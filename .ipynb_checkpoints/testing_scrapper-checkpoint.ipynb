{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weibo_scrapper import weibo_scrapper\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the variables used for testing\n",
    "begin_date ='2018-12-28'\n",
    "end_date = '2019-1-4'\n",
    "username = 'ruoyzhang@outlook.com'\n",
    "password = 'Cyfloel1.1.'\n",
    "search_keyword = '人工智能'\n",
    "save_dir = '../master_thesis_data/weibo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_this = weibo_scrapper(username=username, password=password,\n",
    "                            save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 'https://www.weibo.com/login.php'\n",
      "inputting username and password\n",
      "login successful\n",
      "End of login\n"
     ]
    }
   ],
   "source": [
    "scrape_this.login_weibo()\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "navigating to Weibo adv search page\n",
      "waiting for page to load ...\n",
      "specifying search criterion\n",
      "waiting for results to load ...\n",
      "there are  50  of results\n",
      "now scrapping ...\n",
      "page  1  complete\n",
      "page  2  complete\n",
      "page  3  complete\n",
      "page  4  complete\n",
      "page  5  complete\n",
      "page  6  complete\n",
      "page  7  complete\n",
      "page  8  complete\n",
      "page  9  complete\n",
      "page  10  complete\n",
      "page  11  complete\n",
      "page  12  complete\n",
      "page  13  complete\n",
      "page  14  complete\n",
      "page  15  complete\n",
      "page  16  complete\n",
      "page  17  complete\n",
      "page  18  complete\n",
      "page  19  complete\n",
      "page  20  complete\n",
      "page  21  complete\n",
      "page  22  complete\n",
      "page  23  complete\n",
      "page  24  complete\n",
      "page  25  complete\n",
      "page  26  complete\n",
      "page  27  complete\n",
      "page  28  complete\n",
      "page  29  complete\n",
      "page  30  complete\n",
      "page  31  complete\n",
      "page  32  complete\n",
      "page  33  complete\n",
      "page  34  complete\n",
      "page  35  complete\n",
      "page  36  complete\n",
      "page  37  complete\n",
      "page  38  complete\n",
      "page  39  complete\n",
      "page  40  complete\n",
      "page  41  complete\n",
      "page  42  complete\n",
      "page  43  complete\n",
      "page  44  complete\n",
      "page  45  complete\n",
      "page  46  complete\n",
      "page  47  complete\n",
      "page  48  complete\n",
      "page  49  complete\n",
      "all pages scrapped, now saving ...\n",
      "save complete, files can be found at:  ../master_thesis_data/weibo\n",
      "time lapsed for 50 pages:  8.849363736311595\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "scrape_this.scrape(begin_date=begin_date, end_date=end_date, search_keyword=search_keyword)\n",
    "end = time.time()\n",
    "time_lapsed = (end - begin)/60\n",
    "print('time lapsed for 50 pages: ', str(time_lapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok on a GUI enabled machine with my shitty internet connection, in order to scrape 1 week of data we'll need 9 mins of exec time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to gather 5 years of data, we'll need 39.0 hours\n"
     ]
    }
   ],
   "source": [
    "print('In order to gather 5 years of data, we\\'ll need',str(9 * 52 * 5 / 60), 'hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to gather 3 years of data, we'll need 23.4 hours\n"
     ]
    }
   ],
   "source": [
    "print('In order to gather 3 years of data, we\\'ll need',str(9 * 52 * 3 / 60), 'hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 1.5 days if we want 5 years of data and a day if we want 3 days of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the data we scrapped are in the correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dirs = os.listdir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4_0_2019_text.pickle', '4_0_2019_date.pickle']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_dir, file_dirs[0]), 'rb') as handle:\n",
    "    tweets = pickle.load(handle)\n",
    "with open(os.path.join(save_dir, file_dirs[1]), 'rb') as handle:\n",
    "    dates = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(934, 934)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets), len(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YUUUUUUS! OK not exactly 1K but close enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{datetime.datetime(2019, 1, 4, 0, 0)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although only tweets of one particular dates were recorded, since there are simply too many tweets, also we need to estimate the proportion of duplicated tweets, let's use this dataset for estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "934it [00:00, 14435.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# rough estimation\n",
    "\n",
    "clusters = []\n",
    "\n",
    "for i, tweet_0 in tqdm(enumerate(tweets)):\n",
    "    if sum([i in clus for clus in clusters]) == 0:\n",
    "        clusters.append([i])\n",
    "    else:\n",
    "        continue\n",
    "    for j,tweet_1 in enumerate(tweets[i+1:]):\n",
    "        if tweet_0 == tweet_1:\n",
    "            clusters[-1].append(i+j+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK since the big majority of the duplicated tweets are exactly the same (C&P job), this will help us out A LOT in the deduplication process, however, as we can see quite clearly, we're running on quite a bit of duplicated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have 580 original tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_clusters = [clus for clus in clusters if len(clus) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 33,\n",
       "         3: 6,\n",
       "         4: 2,\n",
       "         5: 1,\n",
       "         6: 2,\n",
       "         7: 1,\n",
       "         9: 1,\n",
       "         15: 1,\n",
       "         35: 1,\n",
       "         61: 1,\n",
       "         168: 1})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([len(clus) for clus in dup_clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(clus) for clus in dup_clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of duplicated tweets are 50 they represent a volume of 580 tweets\n"
     ]
    }
   ],
   "source": [
    "print('the number of duplicated tweets are', len(dup_clusters), 'they represent a volume of', len(clusters), 'tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should consider removing these duplicated tweets since the big majority of them are bot tweets and can lead to severe bias in our data, esepcially considering their over representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll worry about the deduplication later, let's move on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
