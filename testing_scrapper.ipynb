{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weibo_scrapper import weibo_scrapper\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the variables used for testing\n",
    "begin_date ='2018-12-28'\n",
    "end_date = '2018-12-28'\n",
    "username = 'ruoyzhang@outlook.com'\n",
    "password = 'Cyfloel1.1.'\n",
    "search_keyword = '人工智能'\n",
    "save_dir = '../master_thesis_data/weibo'\n",
    "max_page = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_this = weibo_scrapper(username=username, password=password,\n",
    "                            save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 'https://www.weibo.com/login.php'\n",
      "End of login\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a85bb590e812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscrape_this\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin_weibo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/PythonWorkingDirectory/Master_thesis_scrapping/weibo_scrapper.py\u001b[0m in \u001b[0;36mlogin_weibo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m                         \u001b[0;31m# gavigate to the appropriate page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.weibo.com/login.php\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                         \u001b[0;31m# click on the login button\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scrape_this.login_weibo()\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "navigating to Weibo adv search page\n",
      "waiting for page to load ...\n",
      "specifying search criterion\n",
      "waiting for results to load ...\n",
      "there are  15  of results, the scraper will aquire tweets of the first 15 pages\n",
      "now scrapping ...\n",
      "page  1  complete\n",
      "page  2  complete\n",
      "page  3  complete\n",
      "page  4  complete\n",
      "page  5  complete\n",
      "page  6  complete\n",
      "page  7  complete\n",
      "page  8  complete\n",
      "page  9  complete\n",
      "page  10  complete\n",
      "page  11  complete\n",
      "page  12  complete\n",
      "page  13  complete\n",
      "page  14  complete\n",
      "page  15  complete\n",
      "all pages scrapped, now saving ...\n",
      "save complete, files can be found at:  ../master_thesis_data/weibo\n",
      "time lapsed for 50 pages:  2.465424315134684\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "scrape_this.scrape(begin_date=begin_date, end_date=end_date,\n",
    "                   search_keyword=search_keyword,\n",
    "                  max_page=max_page)\n",
    "end = time.time()\n",
    "time_lapsed = (end - begin)/60\n",
    "print('time lapsed for 50 pages: ', str(time_lapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok on a GUI enabled machine with my shitty internet connection, in order to scrape 1 week of data we'll need 9 mins of exec time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to gather 5 years of data, we'll need 39.0 hours\n"
     ]
    }
   ],
   "source": [
    "print('In order to gather 5 years of data, we\\'ll need',str(9 * 52 * 5 / 60), 'hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to gather 3 years of data, we'll need 23.4 hours\n"
     ]
    }
   ],
   "source": [
    "print('In order to gather 3 years of data, we\\'ll need',str(9 * 52 * 3 / 60), 'hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 1.5 days if we want 5 years of data and a day if we want 3 days of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the data we scrapped are in the correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dirs = os.listdir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4_0_2019_text.pickle', '4_0_2019_date.pickle']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(save_dir, file_dirs[0]), 'rb') as handle:\n",
    "    tweets = pickle.load(handle)\n",
    "with open(os.path.join(save_dir, file_dirs[1]), 'rb') as handle:\n",
    "    dates = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(934, 934)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets), len(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YUUUUUUS! OK not exactly 1K but close enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{datetime.datetime(2019, 1, 4, 0, 0)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although only tweets of one particular dates were recorded, since there are simply too many tweets, also we need to estimate the proportion of duplicated tweets, let's use this dataset for estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "934it [00:00, 14435.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# rough estimation\n",
    "\n",
    "clusters = []\n",
    "\n",
    "for i, tweet_0 in tqdm(enumerate(tweets)):\n",
    "    if sum([i in clus for clus in clusters]) == 0:\n",
    "        clusters.append([i])\n",
    "    else:\n",
    "        continue\n",
    "    for j,tweet_1 in enumerate(tweets[i+1:]):\n",
    "        if tweet_0 == tweet_1:\n",
    "            clusters[-1].append(i+j+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK since the big majority of the duplicated tweets are exactly the same (C&P job), this will help us out A LOT in the deduplication process, however, as we can see quite clearly, we're running on quite a bit of duplicated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have 580 original tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_clusters = [clus for clus in clusters if len(clus) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 33,\n",
       "         3: 6,\n",
       "         4: 2,\n",
       "         5: 1,\n",
       "         6: 2,\n",
       "         7: 1,\n",
       "         9: 1,\n",
       "         15: 1,\n",
       "         35: 1,\n",
       "         61: 1,\n",
       "         168: 1})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([len(clus) for clus in dup_clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(clus) for clus in dup_clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of duplicated tweets are 50 they represent a volume of 580 tweets\n"
     ]
    }
   ],
   "source": [
    "print('the number of duplicated tweets are', len(dup_clusters), 'they represent a volume of', len(clusters), 'tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should consider removing these duplicated tweets since the big majority of them are bot tweets and can lead to severe bias in our data, esepcially considering their over representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll worry about the deduplication later, let's move on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
